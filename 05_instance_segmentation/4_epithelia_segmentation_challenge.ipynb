{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install -c conda-forge matplotlib \n",
    "%conda install -c anaconda scipy\n",
    "!pip install tqdm h5py zarr pillow numpy imgaug==0.4.0 mahotas #imgaug has dependency on previous packages\n",
    "!pip install scikit-image\n",
    "!pip install tensorboard\n",
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EU0Y7airu-EY",
    "outputId": "756bf539-63f9-4f33-dcaf-3e40db176df0"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import Image\n",
    "\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as transforms\n",
    "from torchvision.transforms import functional as func_transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
    "from imgaug.augmentables.heatmaps import HeatmapsOnImage\n",
    "\n",
    "\n",
    "import glob\n",
    "import zarr\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from utils.colormap import *\n",
    "from utils.disc_loss import DiscriminativeLoss\n",
    "from unet_fov import *\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Y5GPQC_u-Ed"
   },
   "outputs": [],
   "source": [
    "# decompress data\n",
    "from shutil import unpack_archive\n",
    "unpack_archive(os.path.join('datasets','data_epithelia.tar.gz'), './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5bq3Jl-h3ru"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJ6blcgvu-Ee"
   },
   "source": [
    "Data\n",
    "-------\n",
    "For the third task we are using a set of images showing epithelia cells\n",
    "\n",
    "All images show epithelia tissue. There is no background in the images, only cell membrane/boundary and cell interior.\n",
    "There are 24 images in the training set, 8 in the validation set and 8 in the test set.\n",
    "\n",
    "Two example samples can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kcm44YAoh3rz",
    "outputId": "d239c1e0-aa59-4f0f-d2aa-3d5e76842895"
   },
   "outputs": [],
   "source": [
    "Image(filename='utils/epithelia.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzP_iLLYvSXj",
    "outputId": "31acc47a-44bf-496d-ac46-28e5875714cc"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "inspect the structure of data\n",
    "\n",
    " └── volumes\n",
    "     ├── gt_affs (2, 256, 256) uint8--> affinities map, 2 channels\n",
    "     ├── gt_fgbg (1, 256, 256) uint8--> foreground and background semantic segmentation\n",
    "     ├── gt_labels (1, 256, 256) uint16--> instance segmentation, each instance has a different integer label\n",
    "     ├── gt_tanh (1, 256, 256) float32-->squared distance transformation \n",
    "     └── raw (1, 256, 256) float32--> raw input for the model\n",
    "'''\n",
    "\n",
    "sample_path = glob.glob(os.path.join(\"data_epithelia\", \"train\", \"*.zarr\"))\n",
    "first_data = zarr.open(sample_path[0], 'r')\n",
    "print(first_data.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sA9Qctghh3r1"
   },
   "source": [
    "#TODO: Create the Epithelia_dataset\n",
    "-------\n",
    "We will create the Epithelia_dataset, a subclass which inherits from torch.utils.data.Dataset.\n",
    "\n",
    "When you just have limited number of data for training, data augmentation is essential to get good results.\n",
    "\n",
    "TODO: Implement the part of **define_augmentation** for training data during training on the fly.Think about what kind of augmentation to use (e.g. flips, rotation, elastic).Use the imgaug library (https://imgaug.readthedocs.io/en/latest/), it provides a very extensive list of available augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LiflgEfAvZ1I"
   },
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, image, target):\n",
    "        #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W)\n",
    "        image = func_transforms.to_tensor(image)\n",
    "        target = torch.as_tensor(np.asarray(target), dtype=torch.float)\n",
    "        return image, target\n",
    "\n",
    "class Epithelia_dataset(Dataset):\n",
    "    '''\n",
    "    (subset of the) kaggle data science bowl 2018 dataset.\n",
    "    The data is loaded from disk on the fly and in parallel using the keras Sequence class.\n",
    "    This enables the use of datasets that would not fit into main memory and dynamic augmentation.\n",
    "    Args:\n",
    "        root_dir (string): Directory with all the images-->\"data_epithelia\"\n",
    "        data_type (string): train/val/test, select subset of images\n",
    "        prediction_type (string): two_class/affinities/sdt\n",
    "        transform (callable, optional): Optional transform to be applied\n",
    "        on a sample.\n",
    "    '''\n",
    "    def __init__(self, root_dir, data_type, prediction_type=\"two_class\",return_filename=False):\n",
    "        self.data_type=data_type\n",
    "        self.samples = glob.glob(os.path.join(root_dir, data_type, \"*.zarr\"))\n",
    "        self.prediction_type = prediction_type\n",
    "        self.return_filename=return_filename\n",
    "        self.define_augmentation()\n",
    "        self.to_tensor = ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = zarr.open(self.samples[idx], 'r')\n",
    "        raw = np.array(data['volumes/raw'])\n",
    "   \n",
    "        if self.prediction_type == \"two_class\":\n",
    "            label = np.array(data['volumes/gt_fgbg'])\n",
    "            label = np.logical_not(label)\n",
    "            label = ndimage.binary_dilation(label, iterations=2)\n",
    "            label = label.astype(np.uint8)\n",
    "        elif self.prediction_type == \"affinities\":\n",
    "            label = np.array(data['volumes/gt_affs'])\n",
    "        elif self.prediction_type == \"sdt\":\n",
    "            label = np.array(data['volumes/gt_tanh'])\n",
    "        elif self.prediction_type == \"metric_learning\":\n",
    "            label = np.array(data['volumes/gt_labels'], dtype=np.int32)   \n",
    "\n",
    "        if self.data_type == \"train\":    \n",
    "            raw = np.transpose(raw, [1,2,0]) # CHW -> HWC\n",
    "            label = np.transpose(label, [1,2,0]) # CHW -> HWC\n",
    "            raw, label = self.augment_sample(raw, label)          \n",
    "            raw = np.transpose(raw, [2,0,1]) # HWC -> CHW\n",
    "            label = np.transpose(label, [2,0,1]) # HWC -> CHW\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        raw = np.transpose(raw, [1,2,0]) # CHW -> HWC\n",
    "        raw, label = self.to_tensor(raw, label)\n",
    "       \n",
    "     \n",
    "      \n",
    "        if self.return_filename==True:\n",
    "            return raw, label, self.samples[idx]\n",
    "        else:\n",
    "            return raw, label\n",
    "    \n",
    "    def define_augmentation(self):\n",
    "      \n",
    "        ###########################################################################\n",
    "        # TODO (optional): Define your augmentation pipeline                      #\n",
    "        ###########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        self.transform = iaa.Identity()\n",
    "\n",
    "        # define self.transfrom by looking into the imgaug package reference\n",
    "        # self.transform = iaa.Sequential([\n",
    "        #     ...,\n",
    "        #     ...,\n",
    "        #    ...\n",
    "        # ], random_order=True)\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "    \n",
    "    def augment_sample(self, raw, labels):\n",
    "        # this code makes sure that the same geometric augmentations are applied\n",
    "        # to both the raw image and the label image\n",
    "        if self.prediction_type in [\"sdt\"]:\n",
    "            labels = HeatmapsOnImage(labels, shape=raw.shape, min_value=-1.0, max_value=1.0)\n",
    "            raw, labels = self.transform(image=raw, heatmaps=labels)\n",
    "        else:\n",
    "            labels = SegmentationMapsOnImage(labels, shape=raw.shape)\n",
    "            raw, labels = self.transform(image=raw, segmentation_maps=labels)\n",
    "            \n",
    "        labels = labels.get_arr()\n",
    "        # some pytorch version have problems with negative indices introduced by e.g. flips\n",
    "        # just copying fixes this\n",
    "        labels = labels.copy()\n",
    "        raw = raw.copy()\n",
    "        \n",
    "        return raw, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPZ2uf0Uh3r4"
   },
   "source": [
    "#TODO: Predefine some conditions for CNN training\n",
    "-------\n",
    "We have three types of labels(\"two_class\",\"affinities\",\"sdt\") to train the CNN.\n",
    "\n",
    "For each case, we should define the corresponding output channel numbers, final activation layer, criterion(loss function).It would be clear to fill in these conditions after you look through the part of code about how we define training process.\n",
    "\n",
    "TODO: Please fill in the missing code and uncomment one of the **prediction_type** to start your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fV065nWcu-El"
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# TODO: Uncomment the prediction_type (and corresponding conditions)      #\n",
    "#       you would like to use for this exercice                           #\n",
    "###########################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# Uncomment to choose one\n",
    "\n",
    "#prediction_type = \"two_class\" # same as fg/bg\n",
    "#prediction_type = \"affinities\"\n",
    "#prediction_type = \"sdt\"\n",
    "#prediction_type = \"metric_learning\"\n",
    "\n",
    "if prediction_type = \"two_class\":\n",
    "    out_channels = \n",
    "    activation = \n",
    "    criterion = \n",
    "    dtype = \n",
    "elif prediction_type == \"affinities\":\n",
    "    out_channels = \n",
    "    activation = \n",
    "    criterion = \n",
    "    dtype = \n",
    "elif prediction_type == \"sdt\":\n",
    "    out_channels = \n",
    "    activation = \n",
    "    criterion =\n",
    "    dtype = \n",
    "elif prediction_type == \"metric_learning\":\n",
    "    out_channels = \n",
    "    activation =\n",
    "    criterion = \n",
    "    dtype = \n",
    "else:\n",
    "    raise RuntimeError(\"invalid prediction type\")\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQIVgA7du-Eo"
   },
   "source": [
    "Create our input datasets, ground truth labels are chosen depending on the type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_Afeifju-Eq"
   },
   "outputs": [],
   "source": [
    "train_data = Epithelia_dataset(\"data_epithelia\", \"train\", prediction_type=prediction_type)\n",
    "val_data = Epithelia_dataset(\"data_epithelia\", \"val\", prediction_type=prediction_type)\n",
    "test_data = Epithelia_dataset(\"data_epithelia\", \"test\", prediction_type=prediction_type)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSn290Imu-Er"
   },
   "source": [
    "Let's have a look at some of the raw data and labels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrESlSsEu-Et",
    "outputId": "c9195d85-4df4-43ef-d4fb-431f5973c2fd"
   },
   "outputs": [],
   "source": [
    "# repeatedly execute this cell to get different images\n",
    "dataiter=iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "rnd = random.randrange(len(images)) #train_loader has batch_size=8\n",
    "image = images[rnd]\n",
    "label = labels[rnd]\n",
    "\n",
    "if prediction_type == \"affinities\":\n",
    "    label = label[0] + label[1] # affinities-type label has two channels\n",
    "\n",
    "fig=plt.figure(figsize=(12, 8))\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(np.squeeze(image), cmap='gray')\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(np.squeeze(label), cmap='gist_earth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfXkpfyTu-Et"
   },
   "source": [
    "#TODO: Define our U-Net\n",
    "==============\n",
    "As before, we define our neural network architecture and can choose the depth and number of feature maps at the first convolution.\n",
    "\n",
    "This neural network is composed by stacking one UNet instance and one convolution layer. The UNet Class is defined in the **unet_fov.py** file and we use the default setting that the number of feature maps of the UNet insatnce output will be eqaul to the number of feature maps at the first convolution. Then we use one more convolution layer with kernel_size=1 to generate the final output with number of feature maps equal to what we want.\n",
    "\n",
    "For the meaning of parameters of the UNet class, please refer to the **unet_fov.py** file.\n",
    "\n",
    "#TODO: Please fill in the missing part of code about how to define the **net**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NF-YHXVdu-Et",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "width = 256\n",
    "height = 256\n",
    "\n",
    "###########################################################################\n",
    "# TODO: Define the net and uncomment the following code                   #\n",
    "# Please define a UNet which use same padding                             #\n",
    "###########################################################################\n",
    "'''\n",
    "The explanation of the arguments for the UNet instances are shown below. \n",
    "For more detailed instruction, please look at unet_fov.py.\n",
    "Args:\n",
    "\n",
    "    in_channels:\n",
    "\n",
    "        The number of input channels.\n",
    "\n",
    "    num_fmaps:\n",
    "\n",
    "        The number of feature maps in the first layer. This is also the\n",
    "        number of output feature maps. Stored in the ``channels``\n",
    "        dimension.\n",
    "\n",
    "    fmap_inc_factors:\n",
    "\n",
    "        By how much to multiply the number of feature maps between\n",
    "        layers. If layer 0 has ``k`` feature maps, layer ``l`` will\n",
    "        have ``k*fmap_inc_factor**l``.\n",
    "\n",
    "    downsample_factors:\n",
    "\n",
    "        List of tuples ``(y, x)`` to use to down- and up-sample the\n",
    "        feature maps between layers.\n",
    "\n",
    "\n",
    "    activation:\n",
    "\n",
    "        Which activation to use after a convolution. Accepts the name\n",
    "        of any tensorflow activation function (e.g., ``ReLU`` for\n",
    "        ``torch.nn.ReLU``).\n",
    "\n",
    "    constant_upsample (optional):\n",
    "\n",
    "        If set to true, perform a constant upsampling instead of a\n",
    "        transposed convolution in the upsampling layers.\n",
    "'''\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "#net = torch.nn.Sequential(\n",
    "#     UNet(in_channels=,\n",
    "#     num_fmaps=,\n",
    "#     fmap_inc_factors=,\n",
    "#     downsample_factors=,\n",
    "#     activation='ReLU',\n",
    "#     padding=,\n",
    "#     constant_upsample=\n",
    "#     ),\n",
    "#     torch.nn.Conv2d(in_channels= , out_channels=out_channels, kernel_size=1, padding=0, bias=True))\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################\n",
    "\n",
    "receptive_field, _ = net[0].get_fov()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "net.to(device)\n",
    "\n",
    "print(\"Receptive field of view: {}\".format(receptive_field))\n",
    "summary(net,(1,height,width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcGx68fhu-Eu"
   },
   "source": [
    "#TODO: Training\n",
    "=======\n",
    "\n",
    "Before we have already defined criterion for different prediction types.\n",
    "Now we first define the optimizer(try playing with the learning rate, a higher learning rate can lead to faster training, but also to divergence or lower performance).\n",
    "\n",
    "The train function has already been implemented. Our default setting is\n",
    "\n",
    "1.calculate loss, pixel-wise accuracy, precision and recall four metrics when prediction_type==\"two_class\", \n",
    "\n",
    "2.calculate loss and pixel-wise accuracy when prediction_type==\"affinities\",\n",
    "\n",
    "3.calculate loss when prediction_type==\"sdt\".\n",
    "\n",
    "The corresponding metrics will also be written into the tensorboard. (Note: we don't print the pixel-wise accuracy but you can also add one more line code to print it)\n",
    "\n",
    "#TODO: Please define the optimizer(and the corresponding learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85L4X53uzIzE"
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# TODO: Define the optimzer                                              #\n",
    "###########################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "optimizer = \n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################\n",
    "\n",
    "def do_training(data_loader, tb_writer_train,tb_writer_val, num_epochs=100, start_epoch=0,prediction_type=None):\n",
    "    \n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch+num_epochs):  # loop over the dataset multiple times\n",
    "       \n",
    "        train_loss, train_acc, train_precision, train_recall = do_epoch(data_loader, \"train\", prediction_type)\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_acc, val_precision, val_recall = do_epoch(val_loader, \"val\", prediction_type) \n",
    "        write_train_val_summaries(tb_writer_train, tb_writer_val,\n",
    "                              train_loss, val_loss, train_acc,val_acc,\n",
    "                              train_precision, val_precision,\n",
    "                              train_recall, val_recall, epoch * len(data_loader))\n",
    "        #The weight histogram will be written into the tb_writer_train\n",
    "        net[0].save_weight_histogram(tb_writer_train, epoch)\n",
    "        if prediction_type ==\"two_class\":    \n",
    "            print(f'Epoch {epoch + 1}, train-loss: {train_loss:.4f} - train_precision:{train_precision:.4f} - train_recall: {train_recall:.4f}'+\n",
    "                  f' - val_loss:{val_loss:.4f} -val_precision:{val_precision:.4f} -val_recall:{val_recall:.4f}')\n",
    "            #print(f'Epoch {epoch + 1}, train_acc: {train_acc:.4f} - val_acc: {val_acc}'）\n",
    "        else:\n",
    "            print(f'Epoch {epoch + 1}, train-loss: {train_loss:.4f} - val_loss:{val_loss:.4f}')\n",
    "\n",
    "        \n",
    "           \n",
    "\n",
    "    print('Finished Training')\n",
    "    #save model\n",
    "    torch.save({'epoch': epoch,'model_state_dict': net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': train_loss}, f\"model_epithelia_{epoch}.pth\")\n",
    "    tb_writer_train.close()\n",
    "    tb_writer_val.close()\n",
    "\n",
    "      \n",
    "            \n",
    "def do_epoch(data_loader, mode,prediction_type):\n",
    "    running_loss = 0.0\n",
    "    epoch_pixel_acc=[]\n",
    "    if prediction_type ==\"two_class\":\n",
    "        epoch_precision=[]\n",
    "        epoch_recall=[]\n",
    "    # calling enumerate on data_loader resets it\n",
    "    for i, data in enumerate(data_loader, 1):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = net(inputs)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        if activation:\n",
    "            outputs = activation(outputs)\n",
    "        if mode == \"train\":\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        elif mode == \"val\":\n",
    "            pass\n",
    "        \n",
    "        if prediction_type==\"two_class\":\n",
    "            pixel_acc=compute_metrics(outputs,labels,\"pixel_accuracy\")\n",
    "            precision=compute_metrics(outputs,labels,\"precision\")\n",
    "            recall=compute_metrics(outputs,labels,\"recall\")\n",
    "            epoch_pixel_acc.append(pixel_acc)\n",
    "            epoch_precision.append(precision)\n",
    "            epoch_recall.append(recall)\n",
    "        elif prediction_type==\"affinities\":\n",
    "            pixel_acc=compute_metrics(outputs,labels,\"pixel_accuracy\")\n",
    "            epoch_pixel_acc.append(pixel_acc)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # accumulate statistics\n",
    "        running_loss += loss.item()\n",
    "  \n",
    "\n",
    "    if prediction_type==\"two_class\":\n",
    "        return running_loss/i, np.mean(epoch_pixel_acc),np.mean(epoch_precision),np.mean(epoch_recall)\n",
    "    elif prediction_type==\"affinities\":\n",
    "        return running_loss/i, np.mean(epoch_pixel_acc),None, None\n",
    "    else:\n",
    "        return running_loss/i, None ,None, None\n",
    "\n",
    "def compute_metrics(outputs,labels,key_word=\"pixel_accuracy\"):\n",
    "    '''\n",
    "    for prediction_type == \"two_class\"\n",
    "        \n",
    "        output: torch.tensor, [N, C, H, W], value in (0, 1)\n",
    "        \n",
    "        label: torch.tensor, [N, C, H, W], value in {1, 0}\n",
    "        \n",
    "    key_word:\n",
    "          pixel_accuracy\n",
    "          precision=tp/(tp+fp)\n",
    "          recall=tp/(tp+fn)\n",
    "    '''\n",
    "    metrics_list=[\"pixel_accuracy\",\"percision\",\"recall\"]\n",
    "    if device.type == 'cuda':\n",
    "        outputs=outputs.cpu()\n",
    "        labels=labels.cpu()\n",
    "    pred=torch.ge(outputs, 0.5)\n",
    "\n",
    "    if key_word ==\"pixel_accuracy\":\n",
    "        correct=torch.eq(pred, labels).sum().item()\n",
    "        total=labels.numel() #return total number of elements\n",
    "        metric=correct/total\n",
    "    elif key_word ==\"precision\":\n",
    "        tp=torch.sum(labels[pred==1]).item()\n",
    "        metric=tp/(labels==1).sum()\n",
    "    elif key_word ==\"recall\":\n",
    "        tp=torch.sum(labels[pred==1]).item()\n",
    "        metric=tp/(pred==1).sum()\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Only metrics available are: {metrics_list}\")\n",
    "\n",
    "    return metric\n",
    "    \n",
    "def write_train_val_summaries(tb_writer_train, tb_writer_val,\n",
    "                              train_loss, val_loss, train_acc,val_acc,\n",
    "                              train_precision, val_precision,\n",
    "                              train_recall, val_recall, global_step):\n",
    "    if train_loss is not None:\n",
    "        tb_writer_train.add_scalar('train/val loss',train_loss, global_step)\n",
    "    if train_acc is not None:\n",
    "        tb_writer_train.add_scalar('train/val accuracy',train_acc,global_step)\n",
    "    if train_precision is not None:\n",
    "        tb_writer_train.add_scalar('train/val precision',train_precision, global_step)\n",
    "    if train_recall is not None:\n",
    "        tb_writer_train.add_scalar('train/val recall',train_recall,global_step)\n",
    "    if val_loss is not None:\n",
    "        tb_writer_val.add_scalar('train/val loss',val_loss, global_step)\n",
    "    if val_acc is not None:\n",
    "        tb_writer_val.add_scalar('train/val accuracy',val_acc,global_step)\n",
    "    if val_precision is not None:\n",
    "        tb_writer_val.add_scalar('train/val precision',val_precision, global_step)\n",
    "    if val_recall is not None:\n",
    "        tb_writer_val.add_scalar('train/val recall',val_recall,global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoaPSjEmzVIX"
   },
   "source": [
    "#TODO: Define the training epochs and start to train\n",
    "=======\n",
    "\n",
    "Define the number of epochs and start to train the net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_BLnFlSu-Ew",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# TODO: Define the number of epochs for training                          #\n",
    "###########################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "num_epochs = \n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################\n",
    "start_epoch = 0\n",
    "\n",
    "if start_epoch == 0:\n",
    "    os.makedirs(\"runs\", exist_ok=True)\n",
    "    run_name1=\"runs/run_\" + \"epithelia_segmentation\" + \"_\" + datetime.datetime.now().strftime('%y%m%d_%H%M%S')+\"_train\"\n",
    "    run_name2=\"runs/run_\" + \"epithelia_segmentation\" + \"_\" + datetime.datetime.now().strftime('%y%m%d_%H%M%S')+\"_val\"\n",
    "    tb_writer_train=SummaryWriter(run_name1)#  for train_data\n",
    "    tb_writer_val=SummaryWriter(run_name2)# for val_data\n",
    "\n",
    "do_training(train_loader, tb_writer_train,tb_writer_val, num_epochs=num_epochs, \n",
    "            start_epoch=start_epoch,prediction_type=prediction_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Twevb1NFzf_s"
   },
   "source": [
    "To visualize our results we now use Tensorboard. This is a very useful extension for your browser that let's you look the weights and the metrics over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNUDGxT9u-Ew"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=\"./runs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eC-LKyXlu-Ex"
   },
   "source": [
    "Postprocessing\n",
    "=============\n",
    "\n",
    "In contrast to the semantic segmentation we postprocessing to extract the final segmentation is a bit more involved and consists of x steps:\n",
    "- based on the prediction we define a surface\n",
    "- we extract the maxima from this surface\n",
    "- we use the maxima as seeds in an off-the-shelf watershed algorithm\n",
    "- and mask the result with the foreground\n",
    "The foreground areas covered by the watershed from each seed point correspond to the instances.\n",
    "The resulting instances are then matched to the ground truth instances (at least 50% overlap) to get our final score (averaged over all instances and all test images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwNHra30u-Ex",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from utils.label import *\n",
    "from utils.evaluate import *\n",
    "\n",
    "avg = 0.0\n",
    "\n",
    "#create a test_loader which can return the filename. The filename will be one of the input element of evaluate function\n",
    "special_test_data = Epithelia_dataset(\"data_epithelia\", \"test\", prediction_type=prediction_type,return_filename=True)\n",
    "special_test_loader = DataLoader(special_test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "for idx, data in enumerate (special_test_loader):\n",
    "    inputs, labels, filename = data\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    pred=net(inputs).detach()\n",
    "    if activation:\n",
    "        pred = activation(pred)\n",
    "    # thresholds are \n",
    "    if prediction_type == \"two_class\":\n",
    "        fg_thresh = 0.7\n",
    "        seed_thresh = 0.8 \n",
    "    elif prediction_type == \"sdt\":\n",
    "        fg_thresh = 0.0\n",
    "        seed_thresh = -0.14\n",
    "    elif prediction_type == \"affinities\":\n",
    "        fg_thresh = 0.9\n",
    "        seed_thresh = 0.95\n",
    "    elif prediction_type == \"metric_learning\":\n",
    "        fg_thresh = 0.9\n",
    "        seed_thresh = 0.95\n",
    "    if device.type == 'cuda':\n",
    "        inputs=inputs.cpu()\n",
    "        labels=labels.cpu()\n",
    "        pred=pred.cpu()\n",
    "    else:\n",
    "        pass \n",
    "    image = np.squeeze(inputs.numpy())\n",
    "    gt_labels = np.squeeze(labels.numpy())\n",
    "    pred=np.squeeze(pred.numpy())\n",
    "    if prediction_type == \"affinities\":\n",
    "        gt_labels = gt_labels[0] + gt_labels[1]\n",
    "    \n",
    "    if prediction_type ==\"two_class\":\n",
    "        thresh = pred >= fg_thresh\n",
    "        #thresh = 1 - thresh\n",
    "        thresh=~thresh\n",
    "        labelling, _ = ndimage.label(thresh)\n",
    "        surface = pred        \n",
    "    else:\n",
    "        labelling, surface = label(pred, prediction_type, fg_thresh=fg_thresh, seed_thresh=seed_thresh)\n",
    "        if prediction_type == \"affinities\":\n",
    "            gt_labels = 1 - gt_labels\n",
    "        \n",
    "    ap, precision, recall, tp, fp, fn = evaluate(labelling,filename[0])\n",
    "    avg += ap\n",
    "    labelling = labelling.astype(np.uint8)\n",
    "\n",
    "    print(f\"average precision: {ap}, precision: {precision}, recall: {recall}\")\n",
    "    print(f\"true positives: {tp}, false positives: {fp}, false negatives: {fn}\")\n",
    "    if prediction_type == \"metric_learning\":\n",
    "        surface = surface+np.abs(np.min(surface, axis=(1,2)))[:,np.newaxis,np.newaxis]\n",
    "        surface /= np.max(surface, axis=(1,2))[:,np.newaxis,np.newaxis]\n",
    "        surface = np.transpose(surface, (1,2,0))\n",
    "      \n",
    "    fig=plt.figure(figsize=(16, 8))\n",
    "    ax = fig.add_subplot(1, 4, 1)\n",
    "    ax.set_title(\"raw\")\n",
    "    plt.imshow(np.squeeze(image))\n",
    "    ax = fig.add_subplot(1, 4, 2)\n",
    "    ax.set_title(\"gt labels\")\n",
    "    plt.imshow(np.squeeze(gt_labels))\n",
    "    \n",
    "    ax = fig.add_subplot(1, 4, 3)\n",
    "    ax.set_title(\"prediction\")\n",
    "    plt.imshow(np.squeeze(surface))\n",
    "    ax = fig.add_subplot(1, 4, 4)\n",
    "    ax.set_title(\"pred segmentation\")\n",
    "    plt.imshow(np.squeeze(labelling), cmap=rand_cmap)\n",
    "\n",
    "    plt.show()\n",
    "avg /= (idx+1)\n",
    "print(\"average precision on test set: {}\".format(avg))\n",
    "\n",
    "\n",
    "\n",
    "if prediction_type == \"two_class\":\n",
    "    epoch_precision=0.0\n",
    "    epoch_recall=0.0\n",
    "    for idx, data in enumerate (test_loader,1):\n",
    "        inputs,labels=data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs=net(inputs)\n",
    "        precision=compute_metrics(outputs,labels,\"precision\")\n",
    "        recall=compute_metrics(outputs,labels,\"recall\")\n",
    "        epoch_precision+=precision\n",
    "        epoch_recall+=recall\n",
    "    print(f\"boundary precision: {epoch_precision/idx}, recall: {epoch_recall/idx}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "epithelia_segmentation_challenge.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
